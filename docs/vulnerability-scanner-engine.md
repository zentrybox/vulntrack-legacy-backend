# VulnTrack Vulnerability Scanner Engine Documentation

## 1. Introduction

The **VulnTrack Vulnerability Scanner Engine** is a modern, minimalistic backend designed for orchestrating vulnerability scans, AI-powered analysis, and asynchronous processing, using Supabase as the single source of truth for all data. It is built for reliability, extensibility, and seamless integration with modern frontend stacks and cloud-native workflows.

---

## 2. Architecture Overview

- **Stateless Python backend** (FastAPI)
- **Supabase** as the only data store (CRUD, users, scan results, notifications, etc.)
- **No local database or ORM**: all data operations are RESTful via httpx
- **AI integration**: Google Gemini for executive summaries, remediation, and scan enrichment
- **Asynchronous processing**: Celery + Redis for robust background task execution
- **Queue monitoring**: Flower for real-time task and worker monitoring
- **Containerized**: Docker and docker-compose for reproducible deployments

---

## 3. Key Features

- **Device-agnostic scanning**: Orchestrates vulnerability scans for any device registered in Supabase
- **AI-powered reporting**: Generates executive summaries, remediation steps, and recommendations using Gemini
- **RESTful API**: All endpoints are documented and validated with Pydantic, with OpenAPI/Swagger UI
- **Batch and single scan support**: Trigger scans for one or many devices
- **Robust error handling**: All scan and result insertions are validated and completed, with fallback and logging
- **Notifications**: Inserts notifications for users when scans complete
- **Activity logging**: (Optional) Can log actions in `activity_log_entries` for traceability

---

## 4. Technology Stack

- **FastAPI** (API framework)
- **httpx** (Supabase REST client)
- **Celery** (background task queue)
- **Redis** (Celery broker)
- **Flower** (Celery monitoring)
- **Google Gemini** (AI analysis)
- **Docker** (containerization)
- **Supabase** (Postgres + Auth + Storage)

---

## 5. API Endpoints

All endpoints are prefixed with `/api/v1/`.

### 5.1. Scan Endpoints

- `POST /api/v1/` — Trigger a scan for a single device
- `POST /api/v1/batch` — Trigger batch scans for multiple devices
- `GET /api/v1/scan/{scan_id}` — Get scan status and results
- `GET /api/v1/scan/status` — Get scanning engine status/configuration

#### Request/Response Examples

**Trigger a scan:**
```json
POST /api/v1/
{
  "device_id": "<uuid>"
}
```

**Batch scan:**
```json
POST /api/v1/batch
{
  "device_ids": ["<uuid1>", "<uuid2>"]
}
```

**Scan result (simplified):**
```json
{
  "scan_id": "<uuid>",
  "status": "completed",
  "summary": "AI-generated executive summary...",
  "ai_recommendations": "...",
  "vulnerabilities_found": 2,
  "results": [
    {
      "cve_id": "CVE-2024-3596",
      "severity": "High",
      "description": "...",
      "remediation": "..."
    }
  ]
}
```

---

## 6. Data Flow & Supabase Integration

- **All CRUD operations** (devices, scans, results, notifications) are performed via Supabase REST API using httpx.
- **No local models or migrations**: all schemas are managed in Supabase.
- **Scan results** are inserted into `scan_results`, `vulnerabilities`, and `notifications` tables.
- **AI fields** (summary, recommendations, confidence score) are stored in the appropriate columns in `scans` and `scan_results`.
- **Supabase Auth** is used for user management and notification delivery.

---

## 7. Asynchronous Processing

- **Celery** is used for all scan execution and AI analysis tasks.
- **Redis** is the broker (see docker-compose for service definition).
- **Flower** is available at port 5555 for monitoring tasks and workers.
- **All scan requests** are queued and processed in the background for reliability and scalability.

---

## 8. Deployment & Operations

### 8.1. Docker Compose

A sample `docker-compose.yml` is provided with services for:
- API (FastAPI)
- Worker (Celery)
- Flower (monitoring)
- Redis (broker)

### 8.2. Environment Variables

All sensitive and environment-specific variables are managed via `.env` and mapped in docker-compose:
- `SUPABASE_URL`, `SUPABASE_KEY`
- `GEMINI_API_KEY`
- `BRAVE_SEARCH_API_KEY`, `BRAVE_SEARCH_BASE_URL`
- `REDIS_URL`

### 8.3. Build & Run

```sh
docker compose up --build
```

- API: http://localhost:8000
- Flower: http://localhost:5555
- Redis: localhost:6379

---

## 9. Extending & Customizing

- **Add new scan types**: Extend the Celery worker and API endpoints
- **Integrate new AI models**: Add new flows in the AI service layer
- **Custom notifications**: Modify notification payloads or add new notification types
- **Audit logging**: Enable and use the `activity_log_entries` table for traceability

---

## 10. Security & Best Practices

- **Never expose service role keys or secrets in the frontend**
- **All data access is via Supabase REST API with proper API keys**
- **Use HTTPS and secure your .env files in production**
- **Monitor Celery and Redis for queue health**

---

## 11. Troubleshooting

- **Scan not completing?** Check worker and Redis logs
- **No AI results?** Check Gemini API key and quota
- **No notifications?** Ensure user_id is set and notifications table is configured
- **400 Bad Request from Supabase?** Check payload fields and types against Supabase schema

---

## 12. References

- [Supabase Docs](https://supabase.com/docs)
- [FastAPI Docs](https://fastapi.tiangolo.com/)
- [Celery Docs](https://docs.celeryq.dev/)
- [Flower Docs](https://flower.readthedocs.io/)
- [Google Gemini](https://ai.google.dev/gemini)
- [httpx](https://www.python-httpx.org/)
- [Docker Compose](https://docs.docker.com/compose/)
